{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4-Qbqh3B1o0"
      },
      "source": [
        "# **KG-LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ8DbjoLYO5c",
        "outputId": "80ca6940-bd41-48f3-81e6-dfdcad7c2b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI91YyWsxleS"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.26.1\n",
        "!pip install sentencepiece\n",
        "!pip install transformers --upgrade\n",
        "!pip install sentencepiece --force-reinstall\n",
        "!pip install -U scikit-learn\n",
        "!pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESX6yoZN-DLj"
      },
      "source": [
        "# *KG-LLM(CoT)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgoJZSnHF_Xo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from collections import deque\n",
        "import csv\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIaibsfsMwUn"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_file = open(\"train2id.txt\", \"r\")\n",
        "output_file = open(\"output.txt\", \"w\")\n",
        "\n",
        "# total number of lines\n",
        "number = int(input_file.readline())\n",
        "\n",
        "nodes = set()\n",
        "\n",
        "graph = {}\n",
        "\n",
        "for i in range(number):\n",
        "    content = input_file.readline()\n",
        "    node1, node2, relation = content.strip().split()\n",
        "\n",
        "    nodes.add(node1)\n",
        "    # nodes.add(node2)\n",
        "\n",
        "    relation = int(relation)\n",
        "\n",
        "    # Check if the first node already exists in the dictionary\n",
        "    if node1 not in graph:\n",
        "        # If not, create a new dictionary for the node\n",
        "        graph[node1] = {}\n",
        "    # Add the neighboring node and the relationship to the dictionary for node1\n",
        "    graph[node1][node2] = relation\n",
        "\n",
        "\n",
        "node_list = list(nodes)\n",
        "node_list2 = list(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TwhNMrw9qD3",
        "outputId": "5ae89f90-2fce-47f3-828d-aa809e45d355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "relation2id = {}\n",
        "\n",
        "with open(\"relation2id.txt\", \"r\") as file:\n",
        "    relations = int(file.readline())\n",
        "    for line in file:\n",
        "        relation, relation_id = line.strip().split(\"\\t\")\n",
        "        relation2id[int(relation_id)] = relation\n",
        "\n",
        "unique_rows = set()\n",
        "\n",
        "# change size if you want, but larger the size, smaller the data\n",
        "size = 30\n",
        "# how many positive and negative data set you want to train, the more dataset, the more time to train\n",
        "total = 60000\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "\n",
        "instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "\n",
        "with open(\"train_data.csv\", mode=\"w\", newline='') as tra:\n",
        "  with open(\"positive_data.csv\", mode=\"w\", newline='') as pos:\n",
        "    with open(\"negative_data.csv\",  mode=\"w\", newline='') as neg:\n",
        "\n",
        "        # Create a CSV writer object and write the headers to the file\n",
        "        writer_pos = csv.DictWriter(pos, fieldnames=fieldnames)\n",
        "        writer_pos.writeheader()\n",
        "\n",
        "        writer_neg = csv.DictWriter(neg, fieldnames=fieldnames)\n",
        "        writer_neg.writeheader()\n",
        "\n",
        "        writer_tra = csv.DictWriter(tra, fieldnames=fieldnames)\n",
        "        writer_tra.writeheader()\n",
        "\n",
        "\n",
        "        def dfs(graph, size):\n",
        "            pos_count = 0\n",
        "            neg_count = 0\n",
        "            times = 0\n",
        "            term = True\n",
        "\n",
        "            while times < total:\n",
        "                visited = set()\n",
        "                kg = []\n",
        "                graph_size = random.randint(2, size)\n",
        "                first_node = random.choice(node_list)\n",
        "                visited.add(first_node)\n",
        "                last_node = \"\"\n",
        "                previous_node = first_node\n",
        "                stack = [first_node]\n",
        "                input_text = \"\"\n",
        "                output_text = \"\"\n",
        "                while len(visited) < graph_size:\n",
        "                    if previous_node not in graph or set(graph[previous_node].keys()).issubset(visited):\n",
        "                        node = random.choice(node_list)\n",
        "                        while node in visited:\n",
        "                            node = random.choice(node_list)\n",
        "                        input_text += 'node_{} not connected with node_{}. '.format(previous_node, node)\n",
        "                        output_text += 'node_{} not connected with node_{} means there is no relationship between node_{} and node_{}. '.format(previous_node, node, previous_node, node)\n",
        "                        visited.add(node)\n",
        "                        previous_node = node\n",
        "                    else:\n",
        "                        node = random.choice(list(graph[previous_node].keys()))\n",
        "                        while node in visited:\n",
        "                            node = random.choice(list(graph[previous_node].keys()))\n",
        "                        relation = graph[previous_node][node]\n",
        "                        r = relation2id[relation]\n",
        "                        input_text += 'node_{} has relation_{} with node_{}. '.format(previous_node, relation, node)\n",
        "                        output_text += 'node_{} has relation_{} with node_{}, means node_{} {} node_{}. '.format(previous_node, relation, node, previous_node, r, node)\n",
        "                        visited.add(node)\n",
        "                        previous_node = node\n",
        "                    if len(visited) == graph_size:\n",
        "                        last_node = previous_node\n",
        "\n",
        "                # input_text += 'Answer the following yes/no question by reasoning step-by-step. Is the first node connnected with the last node?'\n",
        "                was = len(unique_rows)\n",
        "                unique_rows.add(input_text)\n",
        "                if len(unique_rows) > was:\n",
        "                  # if pos_count < int(total/2):\n",
        "                    if first_node in graph and last_node in graph[first_node] and term:\n",
        "                      # if pos_count < int(total/2):\n",
        "                        final_relation = relation2id[graph[first_node][last_node]]\n",
        "                        output_text += 'So node {} {} node {}. The answer is yes.'.format(first_node, final_relation, last_node)\n",
        "                        prompt = 'Is node {} connnected with node {}?'.format(first_node, last_node)\n",
        "                        writer_pos.writerow({'input_text': input_text + instruction + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'input_text': input_text + instruction + prompt, 'output_text': output_text})\n",
        "                        pos_count += 1\n",
        "                        term = False\n",
        "                        times += 1\n",
        "                    elif last_node in graph and first_node in graph[last_node] and term:\n",
        "                      # if pos_count < int(total/2):\n",
        "                        final_relation = relation2id[graph[last_node][first_node]]\n",
        "                        output_text += 'So node {} {} node {}. The answer is yes.'.format(last_node, final_relation, first_node)\n",
        "                        prompt = 'Is node {} connected with node {}?'.format(last_node, first_node)\n",
        "                        writer_pos.writerow({'input_text': input_text + instruction + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'input_text': input_text + instruction + prompt, 'output_text': output_text})\n",
        "                        pos_count += 1\n",
        "                        term = False\n",
        "                        times += 1\n",
        "                  # elif neg_count < int(total//2):\n",
        "                    elif not term:\n",
        "                      # if neg_count < int(total/2):\n",
        "                        output_text += 'So there is no connection between node {} and node {}. The answer is no.'.format(first_node, last_node)\n",
        "                        prompt = 'Is node {} connected with node {}?'.format(first_node, last_node)\n",
        "                        writer_neg.writerow({'input_text': input_text + instruction + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'input_text': input_text + instruction + prompt, 'output_text': output_text})\n",
        "                        neg_count += 1\n",
        "                        term = True\n",
        "                        times += 1\n",
        "                else:\n",
        "                  continue\n",
        "\n",
        "\n",
        "            print(pos_count)\n",
        "            print(neg_count)\n",
        "\n",
        "        dfs(graph, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "895ZvJ8MUhWD",
        "outputId": "14d5a20c-f137-43c4-c244-2ed10bd3dbef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f7d023f2-8a47-4bce-a4a9-c5324649e527\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>output_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6139</th>\n",
              "      <td>node_605 has relation_0 with node_25492. node_...</td>\n",
              "      <td>node_605 has relation_0 with node_25492, means...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41783</th>\n",
              "      <td>node_8439 has relation_0 with node_1873. node_...</td>\n",
              "      <td>node_8439 has relation_0 with node_1873, means...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42034</th>\n",
              "      <td>node_16736 has relation_0 with node_16737. Ans...</td>\n",
              "      <td>node_16736 has relation_0 with node_16737, mea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33259</th>\n",
              "      <td>node_33420 has relation_1 with node_16068. nod...</td>\n",
              "      <td>node_33420 has relation_1 with node_16068, mea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49836</th>\n",
              "      <td>node_29938 has relation_4 with node_24391. Ans...</td>\n",
              "      <td>node_29938 has relation_4 with node_24391, mea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7d023f2-8a47-4bce-a4a9-c5324649e527')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f7d023f2-8a47-4bce-a4a9-c5324649e527 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f7d023f2-8a47-4bce-a4a9-c5324649e527');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              input_text  \\\n",
              "6139   node_605 has relation_0 with node_25492. node_...   \n",
              "41783  node_8439 has relation_0 with node_1873. node_...   \n",
              "42034  node_16736 has relation_0 with node_16737. Ans...   \n",
              "33259  node_33420 has relation_1 with node_16068. nod...   \n",
              "49836  node_29938 has relation_4 with node_24391. Ans...   \n",
              "\n",
              "                                             output_text  \n",
              "6139   node_605 has relation_0 with node_25492, means...  \n",
              "41783  node_8439 has relation_0 with node_1873, means...  \n",
              "42034  node_16736 has relation_0 with node_16737, mea...  \n",
              "33259  node_33420 has relation_1 with node_16068, mea...  \n",
              "49836  node_29938 has relation_4 with node_24391, mea...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1 = pd.read_csv('train_data.csv')\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2bWXZizYwHC",
        "outputId": "f0619e7a-cbb2-48a2-f1e2-5b260df75ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows: 30001\n",
            "Unique rows: 30001\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# Open the CSV file for reading and writing\n",
        "with open('positive_data.csv', 'r+') as csvfile:\n",
        "    # Create a CSV reader object\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Create a set to store unique rows\n",
        "    unique_rows = set()\n",
        "\n",
        "    # Iterate over each row in the CSV file\n",
        "    for row in reader:\n",
        "        # Convert the row to a tuple and add it to the set\n",
        "        unique_rows.add(tuple(row))\n",
        "\n",
        "    # Clear the contents of the file and write the header row\n",
        "    csvfile.seek(0)\n",
        "    csvfile.truncate()\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['input_text', 'output_text'])\n",
        "\n",
        "    num_unique_rows = len(unique_rows)\n",
        "    total_rows = reader.line_num\n",
        "\n",
        "    print(\"Total rows:\", total_rows)\n",
        "    print(\"Unique rows:\", num_unique_rows)\n",
        "\n",
        "    # Write the unique rows to the file\n",
        "    for row in unique_rows:\n",
        "        writer.writerow(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFexauIaY-nQ",
        "outputId": "74a5d0e5-cd9a-41a0-935c-f89df56e071b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows: 30001\n",
            "Unique rows: 30001\n"
          ]
        }
      ],
      "source": [
        "# Open the CSV file for reading and writing\n",
        "with open('negative_data.csv', 'r+') as csvfile:\n",
        "    # Create a CSV reader object\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Create a set to store unique rows\n",
        "    unique_rows = set()\n",
        "\n",
        "    # Iterate over each row in the CSV file\n",
        "    for row in reader:\n",
        "        # Convert the row to a tuple and add it to the set\n",
        "        unique_rows.add(tuple(row))\n",
        "\n",
        "    # Clear the contents of the file and write the header row\n",
        "    csvfile.seek(0)\n",
        "    csvfile.truncate()\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['input_text', 'output_text'])\n",
        "\n",
        "    num_unique_rows = len(unique_rows)\n",
        "    total_rows = reader.line_num\n",
        "\n",
        "    print(\"Total rows:\", total_rows)\n",
        "    print(\"Unique rows:\", num_unique_rows)\n",
        "\n",
        "    # Write the unique rows to the file\n",
        "    for row in unique_rows:\n",
        "        writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sygtjp2h3Za4"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv('positive_data.csv')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Save the test data to a new CSV file\n",
        "test_data.to_csv('pos_test.csv', index=False)\n",
        "\n",
        "# Delete the test data from the original CSV file\n",
        "data.drop(test_data.index, inplace=True)\n",
        "data.to_csv('positive_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xR9dhN_3soN"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv('negative_data.csv')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Save the test data to a new CSV file\n",
        "test_data.to_csv('neg_test.csv', index=False)\n",
        "\n",
        "# Delete the test data from the original CSV file\n",
        "data.drop(test_data.index, inplace=True)\n",
        "data.to_csv('negative_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVDz3jd_3sjr"
      },
      "outputs": [],
      "source": [
        "# combind the training data\n",
        "df1 = pd.read_csv('positive_train.csv')\n",
        "df2 = pd.read_csv('negative_train.csv')\n",
        "combined_df = pd.concat([df1, df2], axis=0)\n",
        "combined_df.to_csv('train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGptJp523-q7"
      },
      "outputs": [],
      "source": [
        "# combind the testing data\n",
        "df1 = pd.read_csv('pos_test.csv')\n",
        "df2 = pd.read_csv('neg_test.csv')\n",
        "combined_df = pd.concat([df1, df2], axis=0)\n",
        "combined_df.to_csv('test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_JFDU6rlKpD"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5vR_fbdZkcp"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load the FLAN-T5 model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base', model_max_length=512)\n",
        "\n",
        "# Define the dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the input and output sequences\n",
        "        input_sequence = self.data.iloc[idx]['input_text']\n",
        "        output_sequence = self.data.iloc[idx]['output_text']\n",
        "\n",
        "        # Add task-specific prefix to input sequence\n",
        "        # input_sequence = 'is first node connect with last node: ' + input_sequence\n",
        "\n",
        "        # Encode the input and output sequences using the T5 tokenizer\n",
        "        input_encoding = tokenizer(input_sequence, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
        "        output_encoding = tokenizer(output_sequence, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
        "\n",
        "        # Get the input IDs, attention mask, and label IDs from the encodings\n",
        "        input_ids = input_encoding['input_ids'].squeeze()\n",
        "        attention_mask = input_encoding['attention_mask'].squeeze()\n",
        "        label_ids = output_encoding['input_ids'].squeeze()\n",
        "\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label_ids': label_ids}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Define the data collator function\n",
        "def data_collator(batch):\n",
        "    input_ids = torch.stack([example['input_ids'] for example in batch])\n",
        "    attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
        "    label_ids = torch.stack([example['label_ids'] for example in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label_ids}\n",
        "\n",
        "# Load the data\n",
        "preprocess_data = pd.read_csv('train.csv')\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_data = preprocess_data.sample(frac=0.8, random_state=1)\n",
        "val_data = preprocess_data.drop(train_data.index)\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = MyDataset(train_data)\n",
        "val_dataset = MyDataset(val_data)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_steps=1000,\n",
        ")\n",
        "\n",
        "# Define the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQn38VSEtJr-"
      },
      "source": [
        "**Link Prediction without ICL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1L36gNHmDd7",
        "outputId": "2330cac7-7f81-4014-81ec-6a931f1ee5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.563\n",
            "F1: 0.8458049886621316\n",
            "AUC: 0.8631112391193037\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/results/checkpoint-3000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "test_df = test_df.sample(1000)\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "    input_text = row['input_text']\n",
        "    expected_output = row['output_text']\n",
        "# input_text = \"node_11324 has relation_1 with node_11323. node_11323 has relation_1 with node_26334. \" + instruction + prompt\n",
        "# expected_output = \"node_11324 has relation_1 with node_11323, means node_11324 _derivationally_related_form node_11323. node_11323 has relation_1 with node_26334, means node_11323 _derivationally_related_form node_26334. So node 11324 _derivationally_related_form node 26334. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "    # 对输入进行编码\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # 进行预测并解码输出\n",
        "    generation_config = {\n",
        "        'max_new_tokens': 1500\n",
        "    }\n",
        "\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "    # outputs = model.generate(input_ids)\n",
        "    # output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(outputs)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(count, output_text.split()[-1])\n",
        "    # count+=1\n",
        "    # print(output_text.strip()[-5:])\n",
        "    # print(output_text)\n",
        "    if len(output_text) > 0:\n",
        "        output_text = output_text.split()[-1]\n",
        "    else:\n",
        "        output_text = 'no.'\n",
        "    # print(output_text)\n",
        "    # preds1.append(output_text)\n",
        "    # expec1.append(expected_output.split()[-1])\n",
        "    # input1.append(input_text.split())\n",
        "\n",
        "    output_text_new = output_text.replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    expected_output_new = expected_output.split()[-1].replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    # print(output_text_new, expected_output_new)\n",
        "\n",
        "    preds1.append(output_text_new)\n",
        "    expec1.append(expected_output_new)\n",
        "    input1.append(input_text.split())\n",
        "\n",
        "    if expected_output_new == output_text_new:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "preds_test1 = np.array([1 if 'no' in label else 0 for label in preds1])\n",
        "expec_test1 = np.array([1 if 'no' in label else 0 for label in expec1])\n",
        "# preds_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in preds])\n",
        "# expec_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in expec])\n",
        "f1 = f1_score(expec_test1, preds_test1)\n",
        "\n",
        "# print(preds[:50])\n",
        "# print(expec[:50])\n",
        "\n",
        "auc = roc_auc_score(expec_test1, preds_test1)\n",
        "\n",
        "print(f\"F1: {f1}\")\n",
        "print(f\"AUC: {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayu5YyHRqdKf"
      },
      "source": [
        "An example of the KG-LLM(CoT) output with CoT reasoning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_cP1cMMqcZH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/results/checkpoint-3000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "input_text = \"node_16643 has relation_0 with node_13198. node_13198 has relation_1 with node_16256. Answer the following yes/no question by reasoning step-by-step. Is node 16643 connnected with node 16256?\"\n",
        "expected_output = \"node_16643 has relation_0 with node_13198, means node_16643 _hypernym node_13198. node_13198 has relation_1 with node_16256, means node_13198 _derivationally_related_form node_16256. So node 16643 _derivationally_related_form node 16256. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "# 对输入进行编码\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# 进行预测并解码输出\n",
        "generation_config = {\n",
        "    'max_new_tokens': 1500\n",
        "}\n",
        "\n",
        "outputs = model.generate(input_ids, **generation_config)\n",
        "\n",
        "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(output_text)\n",
        "print(expected_output)\n",
        "# \"node_16643 has relation_0 with node_13198, means node_16643 _hypernym node_13198. node_13198 has relation_1 with node_16256, means node_13198 _derivationally_related_form node_16256. So node 16643 _derivationally_related_form node 16256. The answer is yes.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1fgo8i5HZ6"
      },
      "source": [
        "**Link Prediction with ICL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22nLezfX5Gmn"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAAkAoP75eno"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('test.csv')\n",
        "count = len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB9kNLmi5MTV"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"train.csv\") as train:\n",
        "  with open(\"context.csv\", mode=\"w\", newline='') as context:\n",
        "      writer_context = csv.DictWriter(context, fieldnames=fieldnames)\n",
        "      writer_context.writeheader()\n",
        "      reader_train = csv.reader(train)\n",
        "      # Iterate over each row in the CSV file\n",
        "      for row in reader_train:\n",
        "          input = row[0]\n",
        "          output = row[1]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "\n",
        "          input = input.split('.')\n",
        "          input = input[:-2]\n",
        "          input = '.'.join(input)\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3:-1]\n",
        "          # relationship = words[3]\n",
        "          # print(words)\n",
        "          words = '.'.join(words)\n",
        "\n",
        "          if c < count and len(input) < 100:\n",
        "            cont = 'Context: {}.'.format(input + '.' + words)\n",
        "            writer_context.writerow({'input_text': cont, 'output_text': words})\n",
        "            c += 1\n",
        "            # print(c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbw507su7yra"
      },
      "outputs": [],
      "source": [
        "# read csv files\n",
        "df1 = pd.read_csv('context.csv')\n",
        "df2 = pd.read_csv('test.csv')\n",
        "\n",
        "# combine dataframes by interleaving rows\n",
        "\n",
        "combined_df = pd.concat([df1, df2]).sort_index(kind='merge')\n",
        "\n",
        "# write to new csv file\n",
        "combined_df.to_csv('icl_link.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "tQmovJCw85ay",
        "outputId": "e021884e-05f5-4c8a-96ea-d3503d82558c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>output_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10890</th>\n",
              "      <td>node_63163 has relation_72 with node_2941. nod...</td>\n",
              "      <td>The answer is no.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>node_55978 has relation_37 with node_24922. Is...</td>\n",
              "      <td>The answer is yes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6210</th>\n",
              "      <td>node_5513 has relation_38 with node_1255. node...</td>\n",
              "      <td>The answer is no.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6675</th>\n",
              "      <td>node_5182 has relation_71 with node_2906. node...</td>\n",
              "      <td>The answer is no.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4980</th>\n",
              "      <td>node_43913 has relation_65 with node_2255. Is ...</td>\n",
              "      <td>The answer is yes.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              input_text         output_text\n",
              "10890  node_63163 has relation_72 with node_2941. nod...   The answer is no.\n",
              "948    node_55978 has relation_37 with node_24922. Is...  The answer is yes.\n",
              "6210   node_5513 has relation_38 with node_1255. node...   The answer is no.\n",
              "6675   node_5182 has relation_71 with node_2906. node...   The answer is no.\n",
              "4980   node_43913 has relation_65 with node_2255. Is ...  The answer is yes."
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('icl_link.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cskB2rbJal0f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/common/home/tc822/Desktop/last/results/checkpoint-3000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "test_df = pd.read_csv('/common/home/tc822/Desktop/last/icl_link.csv')\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "    input_text = row['input_text']\n",
        "    expected_output = row['output_text']\n",
        "# input_text = \"node_11324 has relation_1 with node_11323. node_11323 has relation_1 with node_26334. \" + instruction + prompt\n",
        "# expected_output = \"node_11324 has relation_1 with node_11323, means node_11324 _derivationally_related_form node_11323. node_11323 has relation_1 with node_26334, means node_11323 _derivationally_related_form node_26334. So node 11324 _derivationally_related_form node 26334. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "    # 对输入进行编码\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # 进行预测并解码输出\n",
        "    generation_config = {\n",
        "        'max_new_tokens': 1500\n",
        "    }\n",
        "\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "    # outputs = model.generate(input_ids)\n",
        "    # output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(outputs)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(count, output_text.split()[-1])\n",
        "    # count+=1\n",
        "    # print(output_text.strip()[-5:])\n",
        "    # print(output_text)\n",
        "    if len(output_text) > 0:\n",
        "        output_text = output_text.split()[-1]\n",
        "    else:\n",
        "        output_text = 'no.'\n",
        "    # print(output_text)\n",
        "    # preds1.append(output_text)\n",
        "    # expec1.append(expected_output.split()[-1])\n",
        "    # input1.append(input_text.split())\n",
        "\n",
        "    output_text_new = output_text.replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    expected_output_new = expected_output.split()[-1].replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    # print(output_text_new, expected_output_new)\n",
        "\n",
        "    preds1.append(output_text_new)\n",
        "    expec1.append(expected_output_new)\n",
        "    input1.append(input_text.split())\n",
        "\n",
        "    if expected_output_new == output_text_new:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "preds_test1 = np.array([1 if 'no' in label else 0 for label in preds1])\n",
        "expec_test1 = np.array([1 if 'no' in label else 0 for label in expec1])\n",
        "# preds_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in preds])\n",
        "# expec_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in expec])\n",
        "f1 = f1_score(expec_test1, preds_test1)\n",
        "\n",
        "# print(preds[:50])\n",
        "# print(expec[:50])\n",
        "\n",
        "auc = roc_auc_score(expec_test1, preds_test1)\n",
        "\n",
        "print(f\"F1: {f1}\")\n",
        "print(f\"AUC: {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNYQd-IfbrUS"
      },
      "source": [
        "# *Relation prediction without In-context learning*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC8eNXmRbtE4"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "# context = 'Given node_2075 has relation_4 with node_12648. The relation between the first node and last node is _member_meronym. '\n",
        "# instruction = 'Answer the following multiple-choice question by choosing one of these options: '\n",
        "# option = ''\n",
        "instruction = 'Answer the following question step-by-step. '\n",
        "count = 0\n",
        "\n",
        "question = 'The relationship between the first node and the last node is ?'\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"relation2id.txt\", \"r\") as file:\n",
        "    relations = int(file.readline())\n",
        "    for line in file:\n",
        "        relation, relation_id = line.strip().split(\"\\t\")\n",
        "    #     instruction += relation + ', '\n",
        "    # instruction = instruction[:-2] + '. '\n",
        "\n",
        "with open(\"test.csv\") as test:\n",
        "    with open(\"relation.csv\", mode=\"w\", newline='') as icl:\n",
        "      writer_icl = csv.DictWriter(icl, fieldnames=fieldnames)\n",
        "      writer_icl.writeheader()\n",
        "      reader = csv.reader(test)\n",
        "\n",
        "\n",
        "      for row in reader:\n",
        "          # print(row)\n",
        "          input = row[0]\n",
        "          output = row[1]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "          input = input.split('.')\n",
        "          input = input[:-2]\n",
        "          input = '.'.join(input)\n",
        "          # print(input)\n",
        "          input = instruction + input + '. ' + question\n",
        "          # input = input + '. ' + instruction + question\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "          # print(relationship)\n",
        "          output = output[:-2]\n",
        "          output = '.'.join(output)\n",
        "          # print(output)\n",
        "          output += '. The relationship between the first node and the last node is {}.'.format(relationship)\n",
        "          # print(output)\n",
        "          if len(input) < 550:\n",
        "            writer_icl.writerow({'input_text': input, 'output_text': output})\n",
        "            count+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej-meY0Hb4X-",
        "outputId": "b8f92e9d-5d0f-466d-c4ea-461b7dd62b06"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>output_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1627</th>\n",
              "      <td>Answer the following multiple-choice question ...</td>\n",
              "      <td>node_15105 has relation_0 with node_11747, mea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>Answer the following multiple-choice question ...</td>\n",
              "      <td>node_7016 has relation_0 with node_7017, means...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4801</th>\n",
              "      <td>Answer the following multiple-choice question ...</td>\n",
              "      <td>node_24368 has relation_0 with node_4442, mean...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1816</th>\n",
              "      <td>Answer the following multiple-choice question ...</td>\n",
              "      <td>node_32602 has relation_0 with node_23860, mea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985</th>\n",
              "      <td>Answer the following multiple-choice question ...</td>\n",
              "      <td>node_12341 has relation_4 with node_27052, mea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             input_text  \\\n",
              "1627  Answer the following multiple-choice question ...   \n",
              "356   Answer the following multiple-choice question ...   \n",
              "4801  Answer the following multiple-choice question ...   \n",
              "1816  Answer the following multiple-choice question ...   \n",
              "1985  Answer the following multiple-choice question ...   \n",
              "\n",
              "                                            output_text  \n",
              "1627  node_15105 has relation_0 with node_11747, mea...  \n",
              "356   node_7016 has relation_0 with node_7017, means...  \n",
              "4801  node_24368 has relation_0 with node_4442, mean...  \n",
              "1816  node_32602 has relation_0 with node_23860, mea...  \n",
              "1985  node_12341 has relation_4 with node_27052, mea...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('relation.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLp8Y1c6b-8P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/common/home/tc822/Desktop/last/results/checkpoint-3000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "test_df = pd.read_csv('/common/home/tc822/Desktop/last/relation.csv')\n",
        "test_df = test_df.sample(5)\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "    input_text = row['input_text']\n",
        "    expected_output = row['output_text']\n",
        "# input_text = \"node_11324 has relation_1 with node_11323. node_11323 has relation_1 with node_26334. \" + instruction + prompt\n",
        "# expected_output = \"node_11324 has relation_1 with node_11323, means node_11324 _derivationally_related_form node_11323. node_11323 has relation_1 with node_26334, means node_11323 _derivationally_related_form node_26334. So node 11324 _derivationally_related_form node 26334. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "    # 对输入进行编码\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # 进行预测并解码输出\n",
        "    generation_config = {\n",
        "        'max_new_tokens': 1500\n",
        "    }\n",
        "\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "    # outputs = model.generate(input_ids)\n",
        "    # output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(outputs)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(count, output_text.split()[-1])\n",
        "    # count+=1\n",
        "    # print(output_text.strip()[-5:])\n",
        "    print(output_text)\n",
        "    if len(output_text) > 0:\n",
        "        output_text = output_text.split()[-1]\n",
        "    else:\n",
        "        output_text = 'no.'\n",
        "    print(output_text)\n",
        "    # preds1.append(output_text)\n",
        "    # expec1.append(expected_output.split()[-1])\n",
        "    # input1.append(input_text.split())\n",
        "\n",
        "    output_text_new = output_text.replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    expected_output_new = expected_output.split()[-1].replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    # print(output_text_new, expected_output_new)\n",
        "\n",
        "    preds1.append(output_text_new)\n",
        "    expec1.append(expected_output_new)\n",
        "    input1.append(input_text.split())\n",
        "\n",
        "    if expected_output_new == output_text_new:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "preds_test1 = np.array([1 if 'no' in label else 0 for label in preds1])\n",
        "expec_test1 = np.array([1 if 'no' in label else 0 for label in expec1])\n",
        "# preds_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in preds])\n",
        "# expec_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in expec])\n",
        "f1 = f1_score(expec_test1, preds_test1)\n",
        "\n",
        "# print(preds[:50])\n",
        "# print(expec[:50])\n",
        "\n",
        "auc = roc_auc_score(expec_test1, preds_test1)\n",
        "\n",
        "print(f\"F1: {f1}\")\n",
        "print(f\"AUC: {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7x9nXYpb9os"
      },
      "source": [
        "# *Relation prediction with In-context learning*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L73rlXIY5XPF"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddDmuyZhp-WO"
      },
      "outputs": [],
      "source": [
        "# context = 'Given node_2075 has relation_4 with node_12648. The relation between the first node and last node is _member_meronym. '\n",
        "# instruction = 'Answer the following multiple-choice question by choosing one of these options: '\n",
        "# option = ''\n",
        "instruction = 'Answer the following question step-by-step. '\n",
        "count = 0\n",
        "\n",
        "question = 'The relationship between the first node and the last node is ?'\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "with open(\"relation2id.txt\", \"r\") as file:\n",
        "    relations = int(file.readline())\n",
        "    for line in file:\n",
        "        relation, relation_id = line.strip().split(\"\\t\")\n",
        "    #     instruction += relation + ', '\n",
        "    # instruction = instruction[:-2] + '. '\n",
        "\n",
        "with open(\"test.csv\") as test:\n",
        "    with open(\"test_icl.csv\", mode=\"w\", newline='') as icl:\n",
        "      writer_icl = csv.DictWriter(icl, fieldnames=fieldnames)\n",
        "      writer_icl.writeheader()\n",
        "      reader = csv.reader(test)\n",
        "\n",
        "\n",
        "      for row in reader:\n",
        "          # print(row)\n",
        "          input = row[0]\n",
        "          output = row[1]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "          input = input.split('.')\n",
        "          input = input[:-2]\n",
        "          input = '.'.join(input)\n",
        "          # print(input)\n",
        "          input = instruction + input + '. ' + question\n",
        "          # input = input + '. ' + instruction + question\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "          # print(relationship)\n",
        "          output = output[:-2]\n",
        "          output = '.'.join(output)\n",
        "          # print(output)\n",
        "          output += '. The relationship between the first node and the last node is {}.'.format(relationship)\n",
        "          # print(output)\n",
        "          if len(input) < 550:\n",
        "            writer_icl.writerow({'input_text': input, 'output_text': output})\n",
        "            count+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvw4wJx803GZ"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "\n",
        "with open(\"train.csv\") as train:\n",
        "  with open(\"context.csv\", mode=\"w\", newline='') as context:\n",
        "      writer_context = csv.DictWriter(context, fieldnames=fieldnames)\n",
        "      writer_context.writeheader()\n",
        "      reader_train = csv.reader(train)\n",
        "      # Iterate over each row in the CSV file\n",
        "      for row in reader_train:\n",
        "          input = row[0]\n",
        "          output = row[1]\n",
        "          if input == 'input_text' or output == 'output_text':\n",
        "            continue\n",
        "\n",
        "          input = input.split('.')\n",
        "          input = input[:-2]\n",
        "          input = '.'.join(input)\n",
        "\n",
        "          output = output.split('.')\n",
        "          words = output[-3].split()\n",
        "          relationship = words[3]\n",
        "\n",
        "          if c < count and len(input) < 50:\n",
        "            cont = 'Context: {}. The relationship between the first node and last node is {}.'.format(input, relationship)\n",
        "            writer_context.writerow({'input_text': cont, 'output_text': relationship})\n",
        "            c += 1\n",
        "            # print(c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFcFffFgCrc1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# read csv files\n",
        "df1 = pd.read_csv('context.csv')\n",
        "df2 = pd.read_csv('test_icl.csv')\n",
        "\n",
        "# combine dataframes by interleaving rows\n",
        "\n",
        "combined_df = pd.concat([df1, df2]).sort_index(kind='merge')\n",
        "\n",
        "# write to new csv file\n",
        "combined_df.to_csv('icl_relation.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "51xxUqU7AEMV",
        "outputId": "79cc0f33-2538-4ccf-b1d3-c0850b28d8e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>output_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Answer the following multiple-choice question ...</td>\n",
              "      <td>node_7670 has relation_0 with node_16080, mean...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2207</th>\n",
              "      <td>Answer the following multiple-choice question ...</td>\n",
              "      <td>node_16100 has relation_2 with node_6195, mean...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4925</th>\n",
              "      <td>Answer the following multiple-choice question ...</td>\n",
              "      <td>node_32920 has relation_0 with node_3721, mean...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7954</th>\n",
              "      <td>Context: node_24739 has relation_0 with node_2...</td>\n",
              "      <td>_hypernym</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>Context: node_3517 has relation_5 with node_69...</td>\n",
              "      <td>_synset_domain_topic_of</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             input_text  \\\n",
              "49    Answer the following multiple-choice question ...   \n",
              "2207  Answer the following multiple-choice question ...   \n",
              "4925  Answer the following multiple-choice question ...   \n",
              "7954  Context: node_24739 has relation_0 with node_2...   \n",
              "2564  Context: node_3517 has relation_5 with node_69...   \n",
              "\n",
              "                                            output_text  \n",
              "49    node_7670 has relation_0 with node_16080, mean...  \n",
              "2207  node_16100 has relation_2 with node_6195, mean...  \n",
              "4925  node_32920 has relation_0 with node_3721, mean...  \n",
              "7954                                          _hypernym  \n",
              "2564                            _synset_domain_topic_of  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df1 = pd.read_csv('context.csv')\n",
        "df1 = pd.read_csv('icl_relation.csv')\n",
        "\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBWJWOWDMrCD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = 'WN18RR/CoT/checkpoint-1000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "test_df = pd.read_csv('WN18RR/CoT/icl_relation.csv')\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "    input_text = row['input_text']\n",
        "    expected_output = row['output_text']\n",
        "# input_text = \"node_11324 has relation_1 with node_11323. node_11323 has relation_1 with node_26334. \" + instruction + prompt\n",
        "# expected_output = \"node_11324 has relation_1 with node_11323, means node_11324 _derivationally_related_form node_11323. node_11323 has relation_1 with node_26334, means node_11323 _derivationally_related_form node_26334. So node 11324 _derivationally_related_form node 26334. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "    # 对输入进行编码\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # 进行预测并解码输出\n",
        "    generation_config = {\n",
        "        'max_new_tokens': 1500\n",
        "    }\n",
        "\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "    # outputs = model.generate(input_ids)\n",
        "    # output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(outputs)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(count, output_text.split()[-1])\n",
        "    # count+=1\n",
        "    # print(output_text.strip()[-5:])\n",
        "    # print(output_text)\n",
        "    if len(output_text) > 0:\n",
        "        output_text = output_text.split()[-1]\n",
        "    else:\n",
        "        output_text = 'no.'\n",
        "    # print(output_text)\n",
        "    # preds1.append(output_text)\n",
        "    # expec1.append(expected_output.split()[-1])\n",
        "    # input1.append(input_text.split())\n",
        "\n",
        "    output_text_new = output_text.replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    expected_output_new = expected_output.split()[-1].replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    # print(output_text_new, expected_output_new)\n",
        "\n",
        "    preds1.append(output_text_new)\n",
        "    expec1.append(expected_output_new)\n",
        "    input1.append(input_text.split())\n",
        "\n",
        "    if expected_output_new == output_text_new:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "preds_test1 = np.array([1 if 'no' in label else 0 for label in preds1])\n",
        "expec_test1 = np.array([1 if 'no' in label else 0 for label in expec1])\n",
        "# preds_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in preds])\n",
        "# expec_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in expec])\n",
        "f1 = f1_score(expec_test1, preds_test1)\n",
        "\n",
        "# print(preds[:50])\n",
        "# print(expec[:50])\n",
        "\n",
        "auc = roc_auc_score(expec_test1, preds_test1)\n",
        "\n",
        "print(f\"F1: {f1}\")\n",
        "print(f\"AUC: {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpEDrGRzCEq7"
      },
      "source": [
        "# *KG-LLM(Standard)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyBCEt9anQMS"
      },
      "outputs": [],
      "source": [
        "input_file = open(\"train2id.txt\", \"r\")\n",
        "output_file = open(\"output.txt\", \"w\")\n",
        "\n",
        "# total number of lines\n",
        "number = int(input_file.readline())\n",
        "\n",
        "nodes = set()\n",
        "\n",
        "graph = {}\n",
        "\n",
        "for i in range(number):\n",
        "    content = input_file.readline()\n",
        "    node1, node2, relation = content.strip().split()\n",
        "\n",
        "    nodes.add(node1)\n",
        "    # nodes.add(node2)\n",
        "\n",
        "    relation = int(relation)\n",
        "\n",
        "    # Check if the first node already exists in the dictionary\n",
        "    if node1 not in graph:\n",
        "        # If not, create a new dictionary for the node\n",
        "        graph[node1] = {}\n",
        "    # Add the neighboring node and the relationship to the dictionary for node1\n",
        "    graph[node1][node2] = relation\n",
        "\n",
        "\n",
        "node_list = list(nodes)\n",
        "node_list2 = list(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guEg-YCzVQh1",
        "outputId": "64124c27-3d5b-44f0-d114-8860c2404d7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "# relation2id = {}\n",
        "\n",
        "# with open(\"relation2id.txt\", \"r\") as file:\n",
        "#     relations = int(file.readline())\n",
        "#     for line in file:\n",
        "#         relation, relation_id = line.strip().split(\"\\t\")\n",
        "#         relation2id[int(relation_id)] = relation\n",
        "\n",
        "unique_rows = set()\n",
        "\n",
        "# change size if you want, but larger the size, smaller the data\n",
        "size = 30\n",
        "# how many positive and negative data set you want to train, the more dataset, the more time to train\n",
        "total = 60000\n",
        "fieldnames = ['input_text', 'output_text']\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "\n",
        "with open(\"train_data.csv\", mode=\"w\", newline='') as tra:\n",
        "  with open(\"positive_data.csv\", mode=\"w\", newline='') as pos:\n",
        "    with open(\"negative_data.csv\",  mode=\"w\", newline='') as neg:\n",
        "\n",
        "        # Create a CSV writer object and write the headers to the file\n",
        "        writer_pos = csv.DictWriter(pos, fieldnames=fieldnames)\n",
        "        writer_pos.writeheader()\n",
        "\n",
        "        writer_neg = csv.DictWriter(neg, fieldnames=fieldnames)\n",
        "        writer_neg.writeheader()\n",
        "\n",
        "        writer_tra = csv.DictWriter(tra, fieldnames=fieldnames)\n",
        "        writer_tra.writeheader()\n",
        "\n",
        "\n",
        "        def dfs(graph, size):\n",
        "            pos_count = 0\n",
        "            neg_count = 0\n",
        "            times = 0\n",
        "            term = True\n",
        "\n",
        "            while times < total:\n",
        "                visited = set()\n",
        "                kg = []\n",
        "                graph_size = random.randint(2, size)\n",
        "                first_node = random.choice(node_list)\n",
        "                visited.add(first_node)\n",
        "                last_node = \"\"\n",
        "                previous_node = first_node\n",
        "                stack = [first_node]\n",
        "                input_text = \"\"\n",
        "                output_text = \"\"\n",
        "                while len(visited) < graph_size:\n",
        "                    if previous_node not in graph or set(graph[previous_node].keys()).issubset(visited):\n",
        "                        node = random.choice(node_list)\n",
        "                        while node in visited:\n",
        "                            node = random.choice(node_list)\n",
        "                        input_text += 'node_{} not connected with node_{}. '.format(previous_node, node)\n",
        "                        # output_text += 'node_{} not connected with node_{} means there is no relationship between node_{} and node_{}. '.format(previous_node, node, previous_node, node)\n",
        "                        visited.add(node)\n",
        "                        previous_node = node\n",
        "                    else:\n",
        "                        node = random.choice(list(graph[previous_node].keys()))\n",
        "                        while node in visited:\n",
        "                            node = random.choice(list(graph[previous_node].keys()))\n",
        "                        relation = graph[previous_node][node]\n",
        "                        # r = relation2id[relation]\n",
        "                        input_text += 'node_{} has relation_{} with node_{}. '.format(previous_node, relation, node)\n",
        "                        # output_text += 'node_{} has relation_{} with node_{}, means node_{} {} node_{}. '.format(previous_node, relation, node, previous_node, r, node)\n",
        "                        visited.add(node)\n",
        "                        previous_node = node\n",
        "                    if len(visited) == graph_size:\n",
        "                        last_node = previous_node\n",
        "\n",
        "                # input_text += 'Answer the following yes/no question by reasoning step-by-step. Is the first node connnected with the last node?'\n",
        "                was = len(unique_rows)\n",
        "                unique_rows.add(input_text)\n",
        "                if len(unique_rows) > was:\n",
        "                  # if pos_count < int(total/2):\n",
        "                    if first_node in graph and last_node in graph[first_node] and term:\n",
        "                      # if pos_count < int(total/2):\n",
        "                        # final_relation = relation2id[graph[first_node][last_node]]\n",
        "                        output_text += 'The answer is yes.'\n",
        "                        prompt = 'Is node {} connnected with node {}?'.format(first_node, last_node)\n",
        "                        writer_pos.writerow({'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        pos_count += 1\n",
        "                        term = False\n",
        "                        times += 1\n",
        "                    elif last_node in graph and first_node in graph[last_node] and term:\n",
        "                      # if pos_count < int(total/2):\n",
        "                        # final_relation = relation2id[graph[last_node][first_node]]\n",
        "                        output_text += 'The answer is yes.'\n",
        "                        prompt = 'Is node {} connected with node {}?'.format(last_node, first_node)\n",
        "                        writer_pos.writerow({'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        pos_count += 1\n",
        "                        term = False\n",
        "                        times += 1\n",
        "                  # elif neg_count < int(total//2):\n",
        "                    elif not term:\n",
        "                      # if neg_count < int(total/2):\n",
        "                        output_text += 'The answer is no.'\n",
        "                        prompt = 'Is node {} connected with node {}?'.format(first_node, last_node)\n",
        "                        writer_neg.writerow({'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        writer_tra.writerow({'input_text': input_text + prompt, 'output_text': output_text})\n",
        "                        neg_count += 1\n",
        "                        term = True\n",
        "                        times += 1\n",
        "                else:\n",
        "                  continue\n",
        "\n",
        "\n",
        "            print(pos_count)\n",
        "            print(neg_count)\n",
        "\n",
        "        dfs(graph, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "SKTjadz1ngVb",
        "outputId": "b39c27e8-be53-4371-dc64-f4553c0e6143"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0ff78d0c-b8e3-4664-a7bc-38743ff855e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>output_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>59443</th>\n",
              "      <td>node_28451 has relation_0 with node_17195. nod...</td>\n",
              "      <td>The answer is no.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18815</th>\n",
              "      <td>node_5873 has relation_5 with node_172. node_1...</td>\n",
              "      <td>The answer is no.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24541</th>\n",
              "      <td>node_5221 has relation_1 with node_23016. node...</td>\n",
              "      <td>The answer is no.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32618</th>\n",
              "      <td>node_18517 has relation_6 with node_34861. Is ...</td>\n",
              "      <td>The answer is yes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26033</th>\n",
              "      <td>node_10117 has relation_0 with node_184. node_...</td>\n",
              "      <td>The answer is no.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ff78d0c-b8e3-4664-a7bc-38743ff855e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ff78d0c-b8e3-4664-a7bc-38743ff855e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ff78d0c-b8e3-4664-a7bc-38743ff855e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              input_text         output_text\n",
              "59443  node_28451 has relation_0 with node_17195. nod...   The answer is no.\n",
              "18815  node_5873 has relation_5 with node_172. node_1...   The answer is no.\n",
              "24541  node_5221 has relation_1 with node_23016. node...   The answer is no.\n",
              "32618  node_18517 has relation_6 with node_34861. Is ...  The answer is yes.\n",
              "26033  node_10117 has relation_0 with node_184. node_...   The answer is no."
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1 = pd.read_csv('train_data.csv')\n",
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDe-JxFKnhfP",
        "outputId": "1f7e6f0a-cc3f-483f-9a2a-ebed6ecb272b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows: 30001\n",
            "Unique rows: 30001\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# Open the CSV file for reading and writing\n",
        "with open('positive_data.csv', 'r+') as csvfile:\n",
        "    # Create a CSV reader object\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Create a set to store unique rows\n",
        "    unique_rows = set()\n",
        "\n",
        "    # Iterate over each row in the CSV file\n",
        "    for row in reader:\n",
        "        # Convert the row to a tuple and add it to the set\n",
        "        unique_rows.add(tuple(row))\n",
        "\n",
        "    # Clear the contents of the file and write the header row\n",
        "    csvfile.seek(0)\n",
        "    csvfile.truncate()\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['input_text', 'output_text'])\n",
        "\n",
        "    num_unique_rows = len(unique_rows)\n",
        "    total_rows = reader.line_num\n",
        "\n",
        "    print(\"Total rows:\", total_rows)\n",
        "    print(\"Unique rows:\", num_unique_rows)\n",
        "\n",
        "    # Write the unique rows to the file\n",
        "    for row in unique_rows:\n",
        "        writer.writerow(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rkdsnq9nmS3",
        "outputId": "29fc75c5-5bd6-44ba-8b85-94c566915ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows: 30001\n",
            "Unique rows: 30001\n"
          ]
        }
      ],
      "source": [
        "# Open the CSV file for reading and writing\n",
        "with open('negative_data.csv', 'r+') as csvfile:\n",
        "    # Create a CSV reader object\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    # Create a set to store unique rows\n",
        "    unique_rows = set()\n",
        "\n",
        "    # Iterate over each row in the CSV file\n",
        "    for row in reader:\n",
        "        # Convert the row to a tuple and add it to the set\n",
        "        unique_rows.add(tuple(row))\n",
        "\n",
        "    # Clear the contents of the file and write the header row\n",
        "    csvfile.seek(0)\n",
        "    csvfile.truncate()\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['input_text', 'output_text'])\n",
        "\n",
        "    num_unique_rows = len(unique_rows)\n",
        "    total_rows = reader.line_num\n",
        "\n",
        "    print(\"Total rows:\", total_rows)\n",
        "    print(\"Unique rows:\", num_unique_rows)\n",
        "\n",
        "    # Write the unique rows to the file\n",
        "    for row in unique_rows:\n",
        "        writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ib-9WPVnqXL"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv('positive_data.csv')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Save the test data to a new CSV file\n",
        "test_data.to_csv('pos_test.csv', index=False)\n",
        "\n",
        "# Delete the test data from the original CSV file\n",
        "data.drop(test_data.index, inplace=True)\n",
        "data.to_csv('positive_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hdfn-dJntz4"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "data = pd.read_csv('negative_data.csv')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Save the test data to a new CSV file\n",
        "test_data.to_csv('neg_test.csv', index=False)\n",
        "\n",
        "# Delete the test data from the original CSV file\n",
        "data.drop(test_data.index, inplace=True)\n",
        "data.to_csv('negative_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxnACnUFnyad"
      },
      "outputs": [],
      "source": [
        "# combind the training data\n",
        "df1 = pd.read_csv('positive_train.csv')\n",
        "df2 = pd.read_csv('negative_train.csv')\n",
        "combined_df = pd.concat([df1, df2], axis=0)\n",
        "combined_df.to_csv('train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfkOriPen9YQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load the FLAN-T5 model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base', model_max_length=512)\n",
        "\n",
        "# Define the dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the input and output sequences\n",
        "        input_sequence = self.data.iloc[idx]['input_text']\n",
        "        output_sequence = self.data.iloc[idx]['output_text']\n",
        "\n",
        "        # Add task-specific prefix to input sequence\n",
        "        # input_sequence = 'is first node connect with last node: ' + input_sequence\n",
        "\n",
        "        # Encode the input and output sequences using the T5 tokenizer\n",
        "        input_encoding = tokenizer(input_sequence, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
        "        output_encoding = tokenizer(output_sequence, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
        "\n",
        "        # Get the input IDs, attention mask, and label IDs from the encodings\n",
        "        input_ids = input_encoding['input_ids'].squeeze()\n",
        "        attention_mask = input_encoding['attention_mask'].squeeze()\n",
        "        label_ids = output_encoding['input_ids'].squeeze()\n",
        "\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label_ids': label_ids}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Define the data collator function\n",
        "def data_collator(batch):\n",
        "    input_ids = torch.stack([example['input_ids'] for example in batch])\n",
        "    attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
        "    label_ids = torch.stack([example['label_ids'] for example in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label_ids}\n",
        "\n",
        "# Load the data\n",
        "preprocess_data = pd.read_csv('train.csv')\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_data = preprocess_data.sample(frac=0.8, random_state=1)\n",
        "val_data = preprocess_data.drop(train_data.index)\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = MyDataset(train_data)\n",
        "val_dataset = MyDataset(val_data)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_steps=1000,\n",
        ")\n",
        "\n",
        "# Define the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link prediction without ICL"
      ],
      "metadata": {
        "id": "tDVbKvY1a0vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/results/checkpoint-3000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "test_df = test_df.sample(1000)\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "    input_text = row['input_text']\n",
        "    expected_output = row['output_text']\n",
        "# input_text = \"node_11324 has relation_1 with node_11323. node_11323 has relation_1 with node_26334. \" + instruction + prompt\n",
        "# expected_output = \"node_11324 has relation_1 with node_11323, means node_11324 _derivationally_related_form node_11323. node_11323 has relation_1 with node_26334, means node_11323 _derivationally_related_form node_26334. So node 11324 _derivationally_related_form node 26334. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "    # 对输入进行编码\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # 进行预测并解码输出\n",
        "    generation_config = {\n",
        "        'max_new_tokens': 1500\n",
        "    }\n",
        "\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "    # outputs = model.generate(input_ids)\n",
        "    # output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(outputs)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(count, output_text.split()[-1])\n",
        "    # count+=1\n",
        "    # print(output_text.strip()[-5:])\n",
        "    # print(output_text)\n",
        "    if len(output_text) > 0:\n",
        "        output_text = output_text.split()[-1]\n",
        "    else:\n",
        "        output_text = 'no.'\n",
        "    # print(output_text)\n",
        "    # preds1.append(output_text)\n",
        "    # expec1.append(expected_output.split()[-1])\n",
        "    # input1.append(input_text.split())\n",
        "\n",
        "    output_text_new = output_text.replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    expected_output_new = expected_output.split()[-1].replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    # print(output_text_new, expected_output_new)\n",
        "\n",
        "    preds1.append(output_text_new)\n",
        "    expec1.append(expected_output_new)\n",
        "    input1.append(input_text.split())\n",
        "\n",
        "    if expected_output_new == output_text_new:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "preds_test1 = np.array([1 if 'no' in label else 0 for label in preds1])\n",
        "expec_test1 = np.array([1 if 'no' in label else 0 for label in expec1])\n",
        "# preds_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in preds])\n",
        "# expec_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in expec])\n",
        "f1 = f1_score(expec_test1, preds_test1)\n",
        "\n",
        "# print(preds[:50])\n",
        "# print(expec[:50])\n",
        "\n",
        "auc = roc_auc_score(expec_test1, preds_test1)\n",
        "\n",
        "print(f\"F1: {f1}\")\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "id": "iPxON12_az5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link prediction with ICL"
      ],
      "metadata": {
        "id": "4QyiaBcsa7Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/common/home/tc822/Desktop/last/results/checkpoint-3000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "test_df = pd.read_csv('/common/home/tc822/Desktop/last/icl_link.csv')\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "    input_text = row['input_text']\n",
        "    expected_output = row['output_text']\n",
        "# input_text = \"node_11324 has relation_1 with node_11323. node_11323 has relation_1 with node_26334. \" + instruction + prompt\n",
        "# expected_output = \"node_11324 has relation_1 with node_11323, means node_11324 _derivationally_related_form node_11323. node_11323 has relation_1 with node_26334, means node_11323 _derivationally_related_form node_26334. So node 11324 _derivationally_related_form node 26334. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "    # 对输入进行编码\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # 进行预测并解码输出\n",
        "    generation_config = {\n",
        "        'max_new_tokens': 1500\n",
        "    }\n",
        "\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "    # outputs = model.generate(input_ids)\n",
        "    # output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(outputs)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(count, output_text.split()[-1])\n",
        "    # count+=1\n",
        "    # print(output_text.strip()[-5:])\n",
        "    # print(output_text)\n",
        "    if len(output_text) > 0:\n",
        "        output_text = output_text.split()[-1]\n",
        "    else:\n",
        "        output_text = 'no.'\n",
        "    # print(output_text)\n",
        "    # preds1.append(output_text)\n",
        "    # expec1.append(expected_output.split()[-1])\n",
        "    # input1.append(input_text.split())\n",
        "\n",
        "    output_text_new = output_text.replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    expected_output_new = expected_output.split()[-1].replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    # print(output_text_new, expected_output_new)\n",
        "\n",
        "    preds1.append(output_text_new)\n",
        "    expec1.append(expected_output_new)\n",
        "    input1.append(input_text.split())\n",
        "\n",
        "    if expected_output_new == output_text_new:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "preds_test1 = np.array([1 if 'no' in label else 0 for label in preds1])\n",
        "expec_test1 = np.array([1 if 'no' in label else 0 for label in expec1])\n",
        "# preds_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in preds])\n",
        "# expec_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in expec])\n",
        "f1 = f1_score(expec_test1, preds_test1)\n",
        "\n",
        "# print(preds[:50])\n",
        "# print(expec[:50])\n",
        "\n",
        "auc = roc_auc_score(expec_test1, preds_test1)\n",
        "\n",
        "print(f\"F1: {f1}\")\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "id": "h7IimwbUa9f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relation prediction without ICL"
      ],
      "metadata": {
        "id": "GNzyZYy8a99N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/common/home/tc822/Desktop/last/results/checkpoint-3000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "test_df = pd.read_csv('/common/home/tc822/Desktop/last/relation.csv')\n",
        "test_df = test_df.sample(5)\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "    input_text = row['input_text']\n",
        "    expected_output = row['output_text']\n",
        "# input_text = \"node_11324 has relation_1 with node_11323. node_11323 has relation_1 with node_26334. \" + instruction + prompt\n",
        "# expected_output = \"node_11324 has relation_1 with node_11323, means node_11324 _derivationally_related_form node_11323. node_11323 has relation_1 with node_26334, means node_11323 _derivationally_related_form node_26334. So node 11324 _derivationally_related_form node 26334. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "    # 对输入进行编码\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # 进行预测并解码输出\n",
        "    generation_config = {\n",
        "        'max_new_tokens': 1500\n",
        "    }\n",
        "\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "    # outputs = model.generate(input_ids)\n",
        "    # output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(outputs)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(count, output_text.split()[-1])\n",
        "    # count+=1\n",
        "    # print(output_text.strip()[-5:])\n",
        "    print(output_text)\n",
        "    if len(output_text) > 0:\n",
        "        output_text = output_text.split()[-1]\n",
        "    else:\n",
        "        output_text = 'no.'\n",
        "    print(output_text)\n",
        "    # preds1.append(output_text)\n",
        "    # expec1.append(expected_output.split()[-1])\n",
        "    # input1.append(input_text.split())\n",
        "\n",
        "    output_text_new = output_text.replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    expected_output_new = expected_output.split()[-1].replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    # print(output_text_new, expected_output_new)\n",
        "\n",
        "    preds1.append(output_text_new)\n",
        "    expec1.append(expected_output_new)\n",
        "    input1.append(input_text.split())\n",
        "\n",
        "    if expected_output_new == output_text_new:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "preds_test1 = np.array([1 if 'no' in label else 0 for label in preds1])\n",
        "expec_test1 = np.array([1 if 'no' in label else 0 for label in expec1])\n",
        "# preds_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in preds])\n",
        "# expec_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in expec])\n",
        "f1 = f1_score(expec_test1, preds_test1)\n",
        "\n",
        "# print(preds[:50])\n",
        "# print(expec[:50])\n",
        "\n",
        "auc = roc_auc_score(expec_test1, preds_test1)\n",
        "\n",
        "print(f\"F1: {f1}\")\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "id": "U8DkdQNGbCzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relation prediction with ICL"
      ],
      "metadata": {
        "id": "EMy1MwumbDFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds1 = []\n",
        "expec1 = []\n",
        "input1 = []\n",
        "\n",
        "# instruction = 'Answer the following yes/no question by reasoning step-by-step. '\n",
        "# # prompt = 'Is the first node connnected with the last node?'\n",
        "# prompt = 'Is the last node connnected with the first node?'\n",
        "\n",
        "\n",
        "\n",
        "model_path = 'WN18RR/CoT/checkpoint-1000'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "test_df = pd.read_csv('WN18RR/CoT/icl_relation.csv')\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "count = 0\n",
        "for index, row in test_df.iterrows():\n",
        "    input_text = row['input_text']\n",
        "    expected_output = row['output_text']\n",
        "# input_text = \"node_11324 has relation_1 with node_11323. node_11323 has relation_1 with node_26334. \" + instruction + prompt\n",
        "# expected_output = \"node_11324 has relation_1 with node_11323, means node_11324 _derivationally_related_form node_11323. node_11323 has relation_1 with node_26334, means node_11323 _derivationally_related_form node_26334. So node 11324 _derivationally_related_form node 26334. The answer is yes.\"\n",
        "    # print(input_text)\n",
        "    # print(expected_output)\n",
        "\n",
        "    # 对输入进行编码\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # 进行预测并解码输出\n",
        "    generation_config = {\n",
        "        'max_new_tokens': 1500\n",
        "    }\n",
        "\n",
        "    outputs = model.generate(input_ids, **generation_config)\n",
        "    # outputs = model.generate(input_ids)\n",
        "    # output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(outputs)\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # print(count, output_text.split()[-1])\n",
        "    # count+=1\n",
        "    # print(output_text.strip()[-5:])\n",
        "    # print(output_text)\n",
        "    if len(output_text) > 0:\n",
        "        output_text = output_text.split()[-1]\n",
        "    else:\n",
        "        output_text = 'no.'\n",
        "    # print(output_text)\n",
        "    # preds1.append(output_text)\n",
        "    # expec1.append(expected_output.split()[-1])\n",
        "    # input1.append(input_text.split())\n",
        "\n",
        "    output_text_new = output_text.replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    expected_output_new = expected_output.split()[-1].replace(\".\", \"\").replace(\"_\", \"\")\n",
        "    # print(output_text_new, expected_output_new)\n",
        "\n",
        "    preds1.append(output_text_new)\n",
        "    expec1.append(expected_output_new)\n",
        "    input1.append(input_text.split())\n",
        "\n",
        "    if expected_output_new == output_text_new:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "preds_test1 = np.array([1 if 'no' in label else 0 for label in preds1])\n",
        "expec_test1 = np.array([1 if 'no' in label else 0 for label in expec1])\n",
        "# preds_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in preds])\n",
        "# expec_test = np.array([0 if 'no' in label else 1 if 'yes' in label else 2 for label in expec])\n",
        "f1 = f1_score(expec_test1, preds_test1)\n",
        "\n",
        "# print(preds[:50])\n",
        "# print(expec[:50])\n",
        "\n",
        "auc = roc_auc_score(expec_test1, preds_test1)\n",
        "\n",
        "print(f\"F1: {f1}\")\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "id": "MvgNiyF1bFQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}